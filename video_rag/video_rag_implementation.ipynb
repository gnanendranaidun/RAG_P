{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Video RAG Implementation - Phase 1: Video Processing\n",
                "\n",
                "This notebook implements the first phase of the Video RAG pipeline based on the research paper. We will process the video to extract multi-modal information (visual, audio/text, scene text) and prepare it for retrieval.\n",
                "\n",
                "### **Steps:**\n",
                "1.  **Video Segmentation**: Extract frames from the video at a fixed sampling rate.\n",
                "2.  **ASR (Automatic Speech Recognition)**: Extract audio, transcribe it using Whisper, and index the text using FAISS.\n",
                "3.  **OCR (Optical Character Recognition)**: Detect text in frames using EasyOCR and index the text using FAISS.\n",
                "4.  **Visual Feature Extraction**: Extract visual embeddings using CLIP for later use (Object Detection keyframe selection)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: torch in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (2.9.1)\n",
                        "Requirement already satisfied: transformers in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (4.57.3)\n",
                        "Requirement already satisfied: accelerate in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (1.12.0)\n",
                        "Requirement already satisfied: openai-whisper in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (20250625)\n",
                        "Requirement already satisfied: easyocr in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (1.7.2)\n",
                        "Requirement already satisfied: sentence-transformers in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (5.2.0)\n",
                        "Requirement already satisfied: faiss-cpu in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (1.13.2)\n",
                        "Requirement already satisfied: opencv-python in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (4.12.0.88)\n",
                        "Requirement already satisfied: numpy in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (2.2.6)\n",
                        "Requirement already satisfied: pillow in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (12.0.0)\n",
                        "Requirement already satisfied: filelock in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from torch) (3.20.1)\n",
                        "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from torch) (4.15.0)\n",
                        "Requirement already satisfied: setuptools in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from torch) (80.9.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from torch) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from torch) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: fsspec>=0.8.5 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from torch) (2025.12.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (2025.11.3)\n",
                        "Requirement already satisfied: requests in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (0.7.0)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
                        "Requirement already satisfied: psutil in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from accelerate) (7.2.0)\n",
                        "Requirement already satisfied: more-itertools in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from openai-whisper) (10.8.0)\n",
                        "Requirement already satisfied: numba in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from openai-whisper) (0.63.1)\n",
                        "Requirement already satisfied: tiktoken in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from openai-whisper) (0.12.0)\n",
                        "Requirement already satisfied: torchvision>=0.5 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (0.24.1)\n",
                        "Requirement already satisfied: opencv-python-headless in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (4.12.0.88)\n",
                        "Requirement already satisfied: scipy in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (1.16.3)\n",
                        "Requirement already satisfied: scikit-image in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (0.26.0)\n",
                        "Requirement already satisfied: python-bidi in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (0.6.7)\n",
                        "Requirement already satisfied: Shapely in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (2.1.2)\n",
                        "Requirement already satisfied: pyclipper in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (1.4.0)\n",
                        "Requirement already satisfied: ninja in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from easyocr) (1.13.0)\n",
                        "Requirement already satisfied: scikit-learn in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from sentence-transformers) (1.8.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
                        "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from numba->openai-whisper) (0.46.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from requests->transformers) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from requests->transformers) (2.6.2)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from requests->transformers) (2025.11.12)\n",
                        "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from scikit-image->easyocr) (2.37.2)\n",
                        "Requirement already satisfied: tifffile>=2022.8.12 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from scikit-image->easyocr) (2025.12.20)\n",
                        "Requirement already satisfied: lazy-loader>=0.4 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from scikit-image->easyocr) (0.4)\n",
                        "Requirement already satisfied: joblib>=1.3.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
                        "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "# Install necessary packages if not already installed\n",
                "!pip install torch transformers accelerate openai-whisper easyocr sentence-transformers faiss-cpu opencv-python numpy pillow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: mps\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import cv2\n",
                "import numpy as np\n",
                "import torch\n",
                "import faiss\n",
                "import whisper\n",
                "import easyocr\n",
                "from PIL import Image\n",
                "from transformers import AutoTokenizer, AutoModel, CLIPProcessor, CLIPModel\n",
                "import pickle\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Configuration\n",
                "VIDEO_PATH = \"test_video.mp4\"\n",
                "DB_DIR = \"db/video_rag_db\"\n",
                "os.makedirs(DB_DIR, exist_ok=True)\n",
                "\n",
                "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Video Segmentation\n",
                "\n",
                "We extract frames from the video. The paper suggests uniformly sampling frames (e.g., 1 frame per second). This reduces redundancy while capturing sufficient temporal information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting frames...\n",
                        "Extracted 77 frames from test_video.mp4\n"
                    ]
                }
            ],
            "source": [
                "def extract_frames(video_path, fps=1.0):\n",
                "    frames = []\n",
                "    timestamps = []\n",
                "    \n",
                "    cap = cv2.VideoCapture(video_path)\n",
                "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
                "    frame_interval = int(video_fps / fps)\n",
                "    \n",
                "    count = 0\n",
                "    while cap.isOpened():\n",
                "        ret, frame = cap.read()\n",
                "        if not ret:\n",
                "            break\n",
                "        \n",
                "        if count % frame_interval == 0:\n",
                "            # Convert BGR to RGB\n",
                "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "            frames.append(frame_rgb)\n",
                "            timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0) # timestamp in seconds\n",
                "        \n",
                "        count += 1\n",
                "    \n",
                "    cap.release()\n",
                "    return frames, timestamps\n",
                "\n",
                "print(\"Extracting frames...\")\n",
                "frames, timestamps = extract_frames(VIDEO_PATH, fps=1.0)\n",
                "print(f\"Extracted {len(frames)} frames from {VIDEO_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Automatic Speech Recognition (ASR)\n",
                "\n",
                "We use **Whisper** to transcribe the audio from the video. The transcripts provide critical context that might be missed visually. We then encode these transcripts using **Contriever** (or a similar SentenceBERT model) and store them in a FAISS index for retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Transcribing audio...\n",
                        "Generated 20 ASR segments.\n",
                        "{'text': \"In Noet Six, we're looking at the seasonal displays that are popping up everywhere these\", 'start': 0.0, 'end': 6.28, 'type': 'ASR'}\n"
                    ]
                }
            ],
            "source": [
                "# Load Whisper Model\n",
                "asr_model = whisper.load_model(\"base\", device=DEVICE)\n",
                "\n",
                "print(\"Transcribing audio...\")\n",
                "result = asr_model.transcribe(VIDEO_PATH)\n",
                "segments = result['segments']\n",
                "\n",
                "# Prepare ASR documents\n",
                "asr_docs = []\n",
                "for seg in segments:\n",
                "    text = seg['text'].strip()\n",
                "    start = seg['start']\n",
                "    end = seg['end']\n",
                "    if text:\n",
                "        asr_docs.append({\n",
                "            \"text\": text,\n",
                "            \"start\": start,\n",
                "            \"end\": end,\n",
                "            \"type\": \"ASR\"\n",
                "        })\n",
                "\n",
                "print(f\"Generated {len(asr_docs)} ASR segments.\")\n",
                "# Example\n",
                "if asr_docs:\n",
                "    print(asr_docs[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Optical Character Recognition (OCR)\n",
                "\n",
                "We use **EasyOCR** to extract text appearing in the video frames. This helps in understanding scene text which is often query-relevant. Like ASR, we index these text segments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running OCR on sampled frames...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/77 [00:00<?, ?it/s]/Users/gnanendranaidun/Desktop/RAG_P/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
                        "  warnings.warn(warn_msg)\n",
                        "100%|██████████| 77/77 [00:26<00:00,  2.92it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found text in 58 frames.\n",
                        "{'text': '13newsnow-com NEW AT 6', 'timestamp': 0.0, 'frame_idx': 0, 'type': 'OCR'}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "reader = easyocr.Reader(['en'], gpu=(DEVICE != 'cpu'))\n",
                "\n",
                "ocr_docs = []\n",
                "\n",
                "print(\"Running OCR on sampled frames...\")\n",
                "for i, frame in enumerate(tqdm(frames)):\n",
                "    timestamp = timestamps[i]\n",
                "    # EasyOCR expects numpy array (RGB or BGR, we have RGB)\n",
                "    results = reader.readtext(frame)\n",
                "    \n",
                "    frame_text = \" \".join([res[1] for res in results])\n",
                "    \n",
                "    if frame_text.strip():\n",
                "        ocr_docs.append({\n",
                "            \"text\": frame_text,\n",
                "            \"timestamp\": timestamp,\n",
                "            \"frame_idx\": i,\n",
                "            \"type\": \"OCR\"\n",
                "        })\n",
                "\n",
                "print(f\"Found text in {len(ocr_docs)} frames.\")\n",
                "if ocr_docs:\n",
                "    print(ocr_docs[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "9945e0c7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'text': '13newsnow-com NEW AT 6', 'timestamp': 0.0, 'frame_idx': 0, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com NEW AT 6', 'timestamp': 0.9676333333333333, 'frame_idx': 1, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com NEW AT 6', 'timestamp': 1.9352666666666667, 'frame_idx': 2, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com NEW AT 6', 'timestamp': 2.9029000000000003, 'frame_idx': 3, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 3.8705333333333334, 'frame_idx': 4, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 4.838166666666667, 'frame_idx': 5, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 5.8058000000000005, 'frame_idx': 6, 'type': 'OCR'}\n",
                        "{'text': '13newsnow: com', 'timestamp': 6.773433333333334, 'frame_idx': 7, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 7.741066666666667, 'frame_idx': 8, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 8.7087, 'frame_idx': 9, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 9.676333333333334, 'frame_idx': 10, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 10.643966666666667, 'frame_idx': 11, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 11.611600000000001, 'frame_idx': 12, 'type': 'OCR'}\n",
                        "{'text': '13newsnowcom', 'timestamp': 12.579233333333335, 'frame_idx': 13, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 13.546866666666668, 'frame_idx': 14, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 14.5145, 'frame_idx': 15, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 15.482133333333334, 'frame_idx': 16, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 16.44976666666667, 'frame_idx': 17, 'type': 'OCR'}\n",
                        "{'text': 'com', 'timestamp': 17.4174, 'frame_idx': 18, 'type': 'OCR'}\n",
                        "{'text': '13newsnowcom', 'timestamp': 19.352666666666668, 'frame_idx': 20, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 20.3203, 'frame_idx': 21, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 21.287933333333335, 'frame_idx': 22, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 22.255566666666667, 'frame_idx': 23, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 23.223200000000002, 'frame_idx': 24, 'type': 'OCR'}\n",
                        "{'text': 'Hanewsmowcom', 'timestamp': 28.061366666666668, 'frame_idx': 29, 'type': 'OCR'}\n",
                        "{'text': 'A3newenowicom Fco', 'timestamp': 29.029, 'frame_idx': 30, 'type': 'OCR'}\n",
                        "{'text': 'Henewenowcom Fo', 'timestamp': 29.996633333333335, 'frame_idx': 31, 'type': 'OCR'}\n",
                        "{'text': 'Aenewsowacom', 'timestamp': 30.964266666666667, 'frame_idx': 32, 'type': 'OCR'}\n",
                        "{'text': \"'Bnewsnowcom 010 Fi\", 'timestamp': 31.931900000000002, 'frame_idx': 33, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 38.705333333333336, 'frame_idx': 40, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com Paradise trees', 'timestamp': 39.67296666666667, 'frame_idx': 41, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com Paradise trees', 'timestamp': 40.6406, 'frame_idx': 42, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com Paradise trees', 'timestamp': 41.60823333333334, 'frame_idx': 43, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 42.57586666666667, 'frame_idx': 44, 'type': 'OCR'}\n",
                        "{'text': 'T3newsnowcom', 'timestamp': 43.5435, 'frame_idx': 45, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 44.51113333333333, 'frame_idx': 46, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 45.47876666666667, 'frame_idx': 47, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 46.446400000000004, 'frame_idx': 48, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 47.414033333333336, 'frame_idx': 49, 'type': 'OCR'}\n",
                        "{'text': '13newsnowcom', 'timestamp': 48.38166666666667, 'frame_idx': 50, 'type': 'OCR'}\n",
                        "{'text': '4', 'timestamp': 53.219833333333334, 'frame_idx': 55, 'type': 'OCR'}\n",
                        "{'text': 'E3newsnowcom', 'timestamp': 54.18746666666667, 'frame_idx': 56, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 55.155100000000004, 'frame_idx': 57, 'type': 'OCR'}\n",
                        "{'text': \"'13ne Ow con\", 'timestamp': 59.99326666666667, 'frame_idx': 62, 'type': 'OCR'}\n",
                        "{'text': '131 snowicom', 'timestamp': 60.9609, 'frame_idx': 63, 'type': 'OCR'}\n",
                        "{'text': 'wsno com', 'timestamp': 61.928533333333334, 'frame_idx': 64, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 62.89616666666667, 'frame_idx': 65, 'type': 'OCR'}\n",
                        "{'text': 'A3newsrow com', 'timestamp': 63.863800000000005, 'frame_idx': 66, 'type': 'OCR'}\n",
                        "{'text': '13newsnow com', 'timestamp': 64.83143333333334, 'frame_idx': 67, 'type': 'OCR'}\n",
                        "{'text': '43newsnow com', 'timestamp': 65.79906666666668, 'frame_idx': 68, 'type': 'OCR'}\n",
                        "{'text': 'T3newsnow com', 'timestamp': 66.7667, 'frame_idx': 69, 'type': 'OCR'}\n",
                        "{'text': '13riewsnowcom', 'timestamp': 67.73433333333334, 'frame_idx': 70, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com', 'timestamp': 68.70196666666666, 'frame_idx': 71, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com 13 abc NEWSNOW', 'timestamp': 69.6696, 'frame_idx': 72, 'type': 'OCR'}\n",
                        "{'text': \"'3newsnow.com 13 abc NEWSNOW\", 'timestamp': 70.63723333333334, 'frame_idx': 73, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com 13 abc NEWSNOW', 'timestamp': 71.60486666666667, 'frame_idx': 74, 'type': 'OCR'}\n",
                        "{'text': '13newsnowcom 13 abc NEWSNOW', 'timestamp': 72.5725, 'frame_idx': 75, 'type': 'OCR'}\n",
                        "{'text': '13newsnow.com 13 abc NEWSNOW', 'timestamp': 73.54013333333333, 'frame_idx': 76, 'type': 'OCR'}\n"
                    ]
                }
            ],
            "source": [
                "for i in ocr_docs:\n",
                "    print(i)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Text Embedding & Indexing (FAISS)\n",
                "\n",
                "We use **Contriever** (or `sentence-transformers/all-MiniLM-L6-v2` as a practical proxy if Contriever is hard to set up immediately, but the paper specifies Contriever). Let's use a standard `sentence-transformers` model which is compliant with the \"visual-aligned auxiliary texts\" concept."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved asr index with 20 documents.\n",
                        "Saved ocr index with 58 documents.\n"
                    ]
                }
            ],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Load efficient retrieval model\n",
                "retriever = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=DEVICE)\n",
                "\n",
                "def build_faiss_index(docs, index_name):\n",
                "    if not docs:\n",
                "        return None, None\n",
                "    \n",
                "    texts = [d['text'] for d in docs]\n",
                "    embeddings = retriever.encode(texts, convert_to_numpy=True)\n",
                "    \n",
                "    # Normalize for cosine similarity (if using IP index)\n",
                "    faiss.normalize_L2(embeddings)\n",
                "    \n",
                "    d = embeddings.shape[1]\n",
                "    index = faiss.IndexFlatIP(d)\n",
                "    index.add(embeddings)\n",
                "    \n",
                "    # Save Index\n",
                "    faiss.write_index(index, os.path.join(DB_DIR, f\"{index_name}.index\"))\n",
                "    \n",
                "    # Save Metadata\n",
                "    with open(os.path.join(DB_DIR, f\"{index_name}_meta.pkl\"), \"wb\") as f:\n",
                "        pickle.dump(docs, f)\n",
                "    \n",
                "    print(f\"Saved {index_name} index with {len(docs)} documents.\")\n",
                "\n",
                "build_faiss_index(asr_docs, \"asr\")\n",
                "build_faiss_index(ocr_docs, \"ocr\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visual Feature Extraction (CLIP)\n",
                "\n",
                "We extract CLIP visual embeddings for every sampled frame. These will be used in Phase 2 for **Object Detection Keyframe Selection** (filtering frames relevant to a query like \"find the red car\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting Visual Embeddings...\n",
                        "Saved visual embeddings with shape torch.Size([77, 768])\n"
                    ]
                }
            ],
            "source": [
                "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
                "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
                "clip_model = CLIPModel.from_pretrained(clip_model_name).to(DEVICE)\n",
                "\n",
                "batch_size = 32\n",
                "visual_embeddings = []\n",
                "\n",
                "print(\"Extracting Visual Embeddings...\")\n",
                "with torch.no_grad():\n",
                "    for i in range(0, len(frames), batch_size):\n",
                "        batch_frames = frames[i : i + batch_size]\n",
                "        # CLIP expects PIL images or pixel values\n",
                "        # We have numpy RGB arrays, convert to PIL or use processor handles numpy too usually\n",
                "        # but explicit PIL is safer\n",
                "        inputs = clip_processor(images=batch_frames, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
                "        \n",
                "        outputs = clip_model.get_image_features(**inputs)\n",
                "        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True) # Normalize\n",
                "        visual_embeddings.append(outputs.cpu())\n",
                "\n",
                "if visual_embeddings:\n",
                "    visual_features = torch.cat(visual_embeddings)\n",
                "    torch.save(visual_features, os.path.join(DB_DIR, \"visual_embeddings.pt\"))\n",
                "    print(f\"Saved visual embeddings with shape {visual_features.shape}\")\n",
                "else:\n",
                "    print(\"No frames to process.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 1 Complete\n",
                "We have successfully:\n",
                "1. Segmented the video.\n",
                "2. Built an ASR Retrieval Database.\n",
                "3. Built an OCR Retrieval Database.\n",
                "4. Saved Visual Embeddings for future Object Detection steps.\n",
                "\n",
                "Next Step: Phase 2 - Query Processing & Retrieval."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
